<!DOCTYPE html>
<html lang="en" data-theme="light">

<head>
    <meta charset="UTF-8">
    <title>Pico Demo</title>

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@picocss/pico@2/css/pico.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/prismjs/themes/prism.min.css">

    <script defer src="https://cdn.jsdelivr.net/npm/prismjs/prism.min.js"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/prismjs/components/prism-python.min.js"></script>

    <style>
        .center-iframe {
            display: block;
            margin: 0 auto;
            height: 600px;
        }
    </style>

</head>

<body>

    <main class="container" id="content">
        <h1>Data Representation in Scikit-Learn</h1>
        <p>
            Machine learning is about creating models from data; for that reason, we'll start by
            discussing how data can be represented. The best way to think about data within
            Scikit-Learn is in terms of tables.
            A basic table is a two-dimensional grid of data, in which the rows represent individual
            elements of the dataset, and the columns represent quantities related to each of
            these elements. For example, consider the Iris dataset, famously analyzed by Ronald
            Fisher in 1936. We can download this dataset in the form of a Pandas DataFrame
            using the Seaborn library.
        </p>

        <pre><code class="language-python">import pandas as pd
import seaborn as sns
import plotly.express as px

iris = sns.load_dataset('iris')
</code></pre>

        <table class="dataframe">
            <thead>
                <tr style="text-align: right;">
                    <th>sepal_length</th>
                    <th>sepal_width</th>
                    <th>petal_length</th>
                    <th>petal_width</th>
                    <th>species</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>5.1</td>
                    <td>3.5</td>
                    <td>1.4</td>
                    <td>0.2</td>
                    <td>setosa</td>
                </tr>
                <tr>
                    <td>4.9</td>
                    <td>3.0</td>
                    <td>1.4</td>
                    <td>0.2</td>
                    <td>setosa</td>
                </tr>
                <tr>
                    <td>4.7</td>
                    <td>3.2</td>
                    <td>1.3</td>
                    <td>0.2</td>
                    <td>setosa</td>
                </tr>
                <tr>
                    <td>4.6</td>
                    <td>3.1</td>
                    <td>1.5</td>
                    <td>0.2</td>
                    <td>setosa</td>
                </tr>
                <tr>
                    <td>5.0</td>
                    <td>3.6</td>
                    <td>1.4</td>
                    <td>0.2</td>
                    <td>setosa</td>
                </tr>
            </tbody>
        </table>

        <p>Here each row of the data refers to a single observed flower, and the number of rows
            is the total number of flowers in the dataset. In general, we will refer to the rows of
            the matrix as samples, and the number of rows as n_samples. <br>
            Likewise, each column of the data refers to a particular quantitative piece of information
            that describes each sample. In general, we will refer to the columns of the matrix
            as features, and the number of columns as n_features.</p>

        <h4>The Features Matrix</h4>
        <p>The table layout makes clear that the information can be thought of as a two-
            dimensional numerical array or matrix, which we will call the features matrix. By convention,
            this matrix is often stored in a variable named X. The features matrix is
            assumed to be two-dimensional, with shape [n_samples, n_features], and is most
            often contained in a NumPy array or a Pandas DataFrame, though some Scikit-Learn
            models also accept SciPy sparse matrices.</p>
        <p>The samples (i.e., rows) always refer to the individual objects described by the dataset.
            For example, a sample might represent a flower, a person, a document, an image, a
            sound file, a video, an astronomical object, or anything else you can describe with a
            set of quantitative measurements.</p>
        <p>The features (i.e., columns) always refer to the distinct observations that describe
            each sample in a quantitative manner. Features are often real-valued, but may be
            Boolean or discrete-valued in some cases.</p>

        <h4>The Target Array</h4>
        <p>In addition to the feature matrix X, we also generally work with a label or target array,
            which by convention we will usually call y. The target array is usually one-
            dimensional, with length n_samples, and is generally contained in a NumPy array or
            Pandas Series. The target array may have continuous numerical values, or discrete
            classes/labels. While some Scikit-Learn estimators do handle multiple target values in
            the form of a two-dimensional, [n_samples, n_targets] target array, we will pri‐
            marily be working with the common case of a one-dimensional target array.</p>
        <p>A common point of confusion is how the target array differs from the other feature
            columns. The distinguishing characteristic of the target array is that it is usually the
            quantity we want to predict from the features: in statistical terms, it is the dependent
            variable. For example, given the preceding data we may wish to construct a model
            that can predict the species of flower based on the other measurements; in this case,
            the species column would be considered the target array.</p>

        <pre><code class="language-python">sns.pairplot(iris, hue='species', height=1.5);</code></pre>
        <figure style="text-align:center;"><img src="./images/iris_pairplot.svg"></figure>

        <p>For use in Scikit-Learn, we will extract the features matrix and target array from the
            DataFrame, which we can do using some of the Pandas DataFrame operations.</p>
        <pre><code class="language-python">X_iris = iris.drop(columns="species")
y_iris = iris['species']
</code></pre>

        <h3>Supervised Learning Example: Iris Classification</h3>
        <p>For this task, we will use a simple generative model known as Gaussian naive Bayes,
            which proceeds by assuming each class is drawn from an axis-aligned Gaussian dis‐
            tribution. Because it is so fast and has no hyperpara‐
            meters to choose, Gaussian naive Bayes is often a good model to use as a baseline
            classification, before exploring whether improvements can be found through more
            sophisticated models.</p>
        <p>We would like to evaluate the model on data it has not seen before, so we will split the
            data into a training set and a testing set. This could be done by hand, but it is more
            convenient to use the train_test_split utility function:</p>

        <pre><code class="language-python">from sklearn.model_selection import train_test_split
Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)</code></pre>

        <p>With the data arranged, we can follow our recipe to predict the labels:</p>

        <pre><code class="language-python">from sklearn.naive_bayes import GaussianNB
model = GaussianNB() 
model.fit(Xtrain, ytrain) 
y_model = model.predict(Xtest) </code></pre>

        <p>Finally, we can use the accuracy_score utility to see the fraction of predicted labels
            that match their true values:</p>

        <pre><code class="language-python">from sklearn.metrics import accuracy_score
accuracy_score(ytest, y_model) # output: 0.9736842105263158</code></pre>

        <p>With an accuracy topping 97%, we see that even this very naive classification algo‐
            rithm is effective for this particular dataset.</p>

        <h3>Unsupervised Learning Example: Iris Dimensionality</h3>
        <p>As an example of an unsupervised learning problem, let’s take a look at reducing the
            dimensionality of the Iris data so as to more easily visualize it. Recall that the Iris data
            is four-dimensional: there are four features recorded for each sample.</p>
        <p>The task of dimensionality reduction centers around determining whether there is a
            suitable lower-dimensional representation that retains the essential features of the
            data. Often dimensionality reduction is used as an aid to visualizing data: after all, it
            is much easier to plot data in two dimensions than in four dimensions or more!</p>
        <p>Here we will use principal component analysis, which is a fast
            linear dimensionality reduction technique. We will ask the model to return two com‐
            ponents—that is, a two-dimensional representation of the data.</p>

        <pre><code class="language-python">from sklearn.decomposition import PCA
model = PCA(n_components=2) 
model.fit(X_iris) 
X_2D = model.transform(X_iris)</code></pre>

        <p>Now let’s plot the results. A quick way to do this is to insert the results into the origi‐
            nal Iris DataFrame, and use Seaborn’s lmplot to show the results.</p>

        <pre><code class="language-python">iris['PCA1'] = X_2D[:, 0]
iris['PCA2'] = X_2D[:, 1]
px.scatter(iris, "PCA1", "PCA2", color="species", width=700, height=600)</code></pre>

        <iframe class="center-iframe" src="./plots/iris_pca.html" width="800px">
        </iframe>

        <p>We see that in the two-dimensional representation, the species are fairly well separa‐
            ted, even though the PCA algorithm had no knowledge of the species labels! This
            suggests to us that a relatively straightforward classification will probably be effective
            on the dataset, as we saw before.</p>

        <h3>Unsupervised Learning Example: Iris Clustering</h3>
        <p>Let’s next look at applying clustering to the Iris data. A clustering algorithm attempts
            to find distinct groups of data without reference to any labels. Here we will use a
            powerful clustering method called a Gaussian mixture model (GMM). A GMM attempts to model the data as a
            collection of
            Gaussian blobs.</p>
        <p>We can fit the Gaussian mixture model as follows:</p>

        <pre><code class=" language-python">from sklearn.mixture import GaussianMixture
model = GaussianMixture(n_components=3, covariance_type='full')
model.fit(X_iris)
y_gmm = model.predict(X_iris)
iris['cluster'] = y_gmm
px.scatter(iris, "PCA1", "PCA2", facet_col="cluster", color="species")</code></pre>

        <iframe class="center-iframe" src="./plots/iris_cluster.html" width="100%"></iframe>

        <p>By splitting the data by cluster number, we see exactly how well the GMM algorithm
            has recovered the underlying labels: the setosa species is separated perfectly within
            cluster 0, while there remains a small amount of mixing between versicolor and vir‐
            ginica. This means that even without an expert to tell us the species labels of the indi‐
            vidual flowers, the measurements of these flowers are distinct enough that we could
            automatically identify the presence of these different groups of species with a simple
            clustering algorithm! This sort of algorithm might further give experts in the field
            clues as to the relationships between the samples they are observing.</p>

    </main>

</body>

</html>